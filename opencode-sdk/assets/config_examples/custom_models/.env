# Custom Models Configuration Example

# Server Configuration
OPENCODE_BASE_URL=http://localhost:4096
OPENCODE_HOSTNAME=127.0.0.1
OPENCODE_PORT=4096

# Default Custom Model
DEFAULT_PROVIDER=custom
DEFAULT_MODEL=llama-3-70b-instruct

# Anthropic (for comparison)
ANTHROPIC_API_KEY=your_anthropic_key

# Custom Model Configuration
CUSTOM_MODEL_ENDPOINT=http://localhost:8080/v1
CUSTOM_MODEL_API_KEY=your_custom_model_key
CUSTOM_MODEL_NAME=llama-3-70b-instruct
CUSTOM_MODEL_MAX_TOKENS=4096
CUSTOM_MODEL_TEMPERATURE=0.7
CUSTOM_MODEL_TOP_P=0.9
CUSTOM_MODEL_FREQUENCY_PENALTY=1.0
CUSTOM_MODEL_PRESENCE_PENALTY=1.0

# Local Model Configuration (Ollama)
LOCAL_MODEL_HOST=localhost
LOCAL_MODEL_PORT=11434
LOCAL_MODEL_NAME=llama3:70b
LOCAL_MODEL_USE_OLLAMA=true

# Hugging Face Configuration
HUGGINGFACE_API_KEY=your_huggingface_key
HUGGINGFACE_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1
HUGGINGFACE_USE_INFERENCE_ENDPOINT=true

# Custom Model Selection Logic
MODEL_SELECTION_STRATEGY=intent_based

# Intent-Based Model Routing
REASONING_MODEL=claude-3-5-sonnet-20241022
CODING_MODEL=llama-3-70b-instruct
CHAT_MODEL=mixtral-8x7b-instruct
SUMMARIZATION_MODEL=claude-3-haiku-20240307

# Performance Tuning
CUSTOM_MODEL_BATCH_SIZE=1
CUSTOM_MODEL_STREAMING=true
CUSTOM_MODEL_TIMEOUT=120000

# Model-Specific Prompts
REASONING_SYSTEM_PROMPT="You are a helpful assistant that provides detailed, step-by-step reasoning."
CODING_SYSTEM_PROMPT="You are an expert programmer. Provide clean, well-commented code."
CHAT_SYSTEM_PROMPT="You are a friendly conversational assistant."

# Model Capabilities Mapping
REASONING_CAPABILITIES=analysis,logic,planning
CODING_CAPABILITIES=code_generation,debugging,code_review
CHAT_CAPABILITIES=conversation,creativity,general_knowledge

# Cost Configuration
CUSTOM_MODEL_COST_PER_TOKEN=0.00001
HUGGINGFACE_COST_PER_TOKEN=0.00002
ANTHROPIC_COST_PER_TOKEN=0.00015

# Logging
LOG_LEVEL=info
LOG_MODEL_PERFORMANCE=true
LOG_MODEL_USAGE=true
LOG_FILE_PATH=./logs/custom-models.log

# Application Settings
APP_NAME=OpenCode Custom Models Integration
NODE_ENV=production
SESSION_TIMEOUT=600000

# Cache Configuration
ENABLE_MODEL_RESPONSE_CACHE=true
CACHE_TTL=3600
CACHE_MAX_SIZE=1000

# Model Health Monitoring
ENABLE_MODEL_HEALTH_CHECKS=true
MODEL_HEALTH_CHECK_INTERVAL=60000

# Fallback Configuration
FALLBACK_TO_CLAUDE=true
FALLBACK_THRESHOLD_ERROR_RATE=0.1